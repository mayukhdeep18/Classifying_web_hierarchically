hardware random number generators robert davies statistics research associates limited http : //statsresearch.co.nz http : //www.robertnz.net 14 october , 2000 return to robertnz.net random number generator page return to sra papers page this is a paper presented to the 15th australian statistics conference in july , 2000 and the 51st conference of the nz statistical association in september , 2000 . the paper is about the testing and the applications of hardware random number generators . part of the work arises from a confidential project so at the present time my software is not available. sorry . a hardware random number generator is an electronic device that plugs into a computer and produces genuine random numbers. this is in contrast to the pseudo-random numbers produced by a random number computer program. several hardware random number generators are available from commercial sources. see my 1997 web page . they are used for generating keys for encryption , winning numbers for lotteries , selecting experimental designs and occasionally for statistical simulations . testing a hardware random generator differs from testing a pseudo-random number generator. in particular , if one knows the design of the generator one can tailor the tests to be appropriate for that design. the intended application may also be important when selecting the tests. on the other hand , some of the tests used for pseudo-random number generators are n't very useful when applied to hardware random number generators . your random number generator might pass the test but this does not necessarily mean that the generator is good . or your generator might fail the test but this does not necessarily mean that the generator is bad . this is from an engineer i was working with . testing a random number generator is different from testing , for example , an arithmetic unit. while the first paragraph might apply to the arithmetic unit the second wouldn  t. for those used to testing deterministic devices , testing hardware random number generators involves new concepts and challenges . hardware random number generator : a physical device that connects to a computer and produces genuine random numbers : a hardware random number generator may use ( for example ) decay times from a radio-active material electrical noise from a resistor or semiconductor a hardware random number generator uses a physical phenomenon such as electrical noise from a resistor or semiconductor diode or the decay of a radioactive material for the initial source of randomness. the electronic circuitry of the generator converts this noise to bits and then assembles these into bytes or words for use by the computer . there may be philosophical question as to whether processes used to generate the random noise are really random  as quantum theory suggests they are. maybe we could calculate the movements of the electrons in the resistor and so model the noise as a deterministic process . this , of course , would impossible , because of huge number of electrons that would be involved even if the processes were really deterministic. so even if god really doesn  t play dice , we can still regard the process as genuinely random , for our purposes . all the commercial hardware random number generators that i have examined use resistor or semiconductor noise as the source of randomness. there is an internet site , http : //www.fourmilab.ch/hotbits , from which you can download bits generated by radioactive decay. i don  t know whether the generator is available commercially. in this paper i will look at just generators using resistor or semiconductor noise. however , some of the principles discussed here will also apply to radioactive generators . here is a picture of one of the commercial hardware random number generators ( from protego in sweden ) . this one plugs into the serial port of a computer. you can see the diode at the right hand end , which i presume is the noise generator. i presume that the rest of the circuitry is an amplifier and limiter. this generator uses the computer  s serial port circuitry to digitise the random noise . used for key word generation in encryption generating winning numbers for lotteries experimental design statistical simulations the major application is for encryption . one wants to generate a keyword that is hard to guess and using a hardware random number generator is probably the most reliable way of doing this . hardware random number generators are also used for selecting winning numbers in lotteries. in particular the new zealand lotteries commission uses one for generating its keno numbers . hardware random number generators are also used for designing for experiments on extra sensory perception ( esp ) . one of the commercial generators was developed explicitly for this purpose . the present day hardware random number generators are really too slow for routine use in statistical simulations although it might be very sensible to use them for cross-checking simulations using pseudo-random number generators . a hardware random number generator generates a series of bits. i will say that they are uniform random if they have expectation 0.5 and are independent. i use the term uniform random since a sequence of n such bits interpreted as a binary number will be uniform on the integers 0 to 2 n -1. my tests are oriented towards seeing how closely the output from the generator is uniform random . for most purposes , we would like the bits ( statisticians call them bernoulli variables  variables taking on the values 0 or 1 ) produced by the generator to be statistically independent and having expectation ( long run average value ) 0.5. i refer to a stream of such bits as being uniform random since a sequence of n such bits interpreted as a binary number would have a uniform distribution on the integers 0 to 2 n -1. some people would describe the bits as being perfectly random or completely random . but i regard this as misleading since , for example , i wouldn  t regard a markov process , from a genuinely random process , for example , as being imperfectly or incompletely random . most of this paper is about testing the hypothesis that the output from a hardware random number generator is indeed , uniform random , or , if it isn  t , describing how it deviates . a hardware random number generator is different from a pseudo-random number generator , which uses a formula for generating the numbers. so we need a different approach to testing. it is a physical device so analyse it like a physical device . then reasonably satisfactory tests are possible . but we need to know about its internal structure . i regard the problem of testing of hardware random number generator as different from that of testing a pseudo-random number generator. in particular , i consider marsaglia  s diehard tests to be generally inappropriate for this purpose. various reasons for this will become apparent throughout this paper. however , the most important is that the diehard tests are designed for 32 bit words , whereas hardware random number generators tend to generate their bits one by one and don  t have a 32 bit structure . the philosophy here is that a hardware random number generator is a physical device , so we will test it like a physical device. that is , look at its structure , decide where the problems are most likely to arise and concentrate on these. but , of course , don  t ignore the possibility of a problem in an area you hadn  t predicted . following this approach , i think a reasonably satisfactory testing regime is possible. i think this is in contrast with testing pseudo-random number generators where the testing process is never really satisfactory . as a footnote , if we are going to test it as a physical device , we need to know about its internal structure or we won  t be able to carry out the most effective testing. this means that the manufacturer has to tell us what is inside ( or we have to take one to bits ) . george marsaglia random numbers fall mainly in the planes diehard tests random number cd-rom http : //stat.fsu.edu/ ~ geo/diehard.html george marsaglia is a well-known random number guru. his paper random numbers fall mainly in the planes showed that if you plotted the results of many pseudo-random number generators in several dimensions [ e.g . plot ( x 1 , x 2 , x 3 ) , ( x 2 , x 3 , x 4 ) , ( x 3 , x 4 , x 5 ) , as points in 3 dimensional space where x 1 , x 2 , ... are the 32 bit words produced by the pseudo-random number generator ] then the points tended to lie on a lattice rather than being uniformly distributed. this resulted a rethink of some of the traditional pseudo-rngs and the development of new ones . marsaglia has developed a series of statistical tests known as the diehard tests principally for testing pseudo-rngs. in a certain sense , these do not constitute a formal test since there are so many tests you must get some significant deviations. my approach is to run the series of tests and look by eye for groups of significant deviations or for single extremely significant deviations . george marsaglia released a cd-rom of random numbers which included series from 3 hardware generators. he claimed that none passed his diehard tests. one of the hardware generators was the tundra/newbridge generator that i have tested extensively and the reason it failed was a programming or data handling error. more about this later. but i think this may have tended to discredit hardware generators . here is a flow diagram of a hypothetical hardware random number generator including both the hardware and software components . the analogue noise source is the noise resistor or semiconductor and the digitiser converts the output of the noise generator into a sequence of digital 0s and 1s. these form the raw output from the generator. these will usually be biased , that is , their expected ( long run average ) value will be different from 0.5. so a corrector is necessary , to bring the expected value closer to 0.5. an example of a simple corrector takes two input bits and outputs the exclusive-or of these two bits . i recommend taking the exclusive-or of the corrector output and the output of a pseudo-random number generator. this serves two purposes ; first to mop up in the remaining deviation from uniform randomness and secondly to guard against bad spots in the hardware generator output or failure of the generator in the middle of , for example , the lottery draw . the letters in the diagram indicate test points. it will not usually be possible for the user to check point a , but the manufacturer may provide some information. see , for example , the protego web site , http : //www.protego.se , for some oscilloscope traces at point a of their generator . i think it is important to be able to test the uncorrected output at point b . if we are limited to point c then testing is significantly less effective. it is most unsatisfactory if we are limited to testing at just points e or f . testing at d , e and f is just to check that all the processes are working correctly. they are important , but not so much a topic of this paper . for a more complex generator the tests considered here will need to be amended. for example , the tundra/newbridge generator has 8 generator units each supplying one bit in a byte. these need to be tested separately using the methods described in this paper. in addition , one also needs to check the independence of the separate streams . as another example , some of the commercial generators have only one generator unit but return their output in blocks of 4 , 8 or 32 bits , for example. at least some of the tests will need to be adapted to take account of this structure. xor corrector take successive pairs of bits in input stream and take exclusive-or ( xor ) of each pair. use this for the output stream. if the bytes in the input stream are independent & have bias e then the bias in the output stream is -2 e 2 . von neumann corrector take pairs as before. if the two bits are different use the first one ; if they are the same throw them away. if the bytes in the input stream are stationary the output stream is unbiased . here are descriptions of two types of correctors used to reduce bias in a stream of random bits . the xor corrector just takes the exclusive-or of pairs of bits and dramatically reduces bias if the original bits are independent . the von neumann corrector also looks at pairs of bits but uses the first one if the bits are different and otherwise throws them away. if the input stream is stationary the output is unbiased. however it may still be auto-correlated if the original stream is auto-correlated. also note that the von neumann corrector will produce a biased output if there is a cycle with period 2 on the input stream  and if the corrector is implemented in hardware it may well interfere with the generator and produce just such a cycle . raw 10 11 00 10 10 01 00 01 10 10 01 01 11 00 10 00 10 10 01 01 11 xor 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 von neumann 1 1 1 0 0 1 1 0 0 1 1 1 0 0 here is an example of the processing by the two correctors. alternate pairs of points in the raw series are coloured blue to help you line up the columns . bias drift short term auto-correlation other short term dependencies discrete frequencies 1/ f noise other non-whiteness bad spots back door here is a list of deviations from uniform randomness that might occur in the raw bits. the generator is likely to be biased , which is why we need the corrector stage. the bias may drift over time. while this will largely be corrected by the corrector , if there is drift , eventually the generator may drift outside the range that can be handled by the corrector or by the amplifiers in the generator. so we need to check for this . if we sample the analogue output too fast , adjacent bits will be correlated . so we need to check for short term auto-correlation. it is possible to have short-term dependencies apart from correlation so it is a good idea to check for other types of short term dependencies. [ throw a fair coin twice and then simulate a third throw by pretending we got an h if the previous two throws were the same and t if they were different. then the three throws are uncorrelated but they are not independent since given any two we can deduce the third. ] it is possible for the generator to pick up external electrical interference. the most obvious effect of this will be discrete frequencies from the electrical mains and the various oscillators present in a computer . semiconductor noise tends to have an excess of low frequency  for very low frequencies the spectrum is proportional to 1/ f to some power near one. even if the generator is using a resistor for generating the noise there will be some semi-conductor noise from the amplifier so we should check for excess low frequency noise . there may be other distortions to the frequency. for example , some circuits for random number generators on the internet show a capacitor between the noise device and the amplifier. this would chop out the low frequency . bad spots are short periods where the generator ceases to work , possibly due to electrical interference or to extreme excursions of the generator overloading part of the circuitry . back door refers to deviations from uniform randomness deliberately introduced by the manufacturer to enable those in the know to guess keywords or win lotteries. suppose instead of being a hardware random number generator the device was really a high quality pseudo-random number generator with a 40 bit seed. it would be hard to detect this from the output but it would be computationally feasible for some-one in the know to search through all the keywords it might generate . defect test relevance bias mean both drift chi-sq on block mean raw short-term autocorrelation autocorrelations 1 to 7 both short-term dependence 4,8,16 bit chi-square & monkey tests both frequency components periodogram & spectral analysis raw 1/ f noise autocorrelation ? raw non-whiteness spectral analysis raw now look at the kind of tests we might use to look for these defects  deviations from random uniformness. we should look at both the raw series of bits ( point b in the flow diagram ) and corrected series ( point c ) . for most of the tests i have used 10 megabyte samples but on occasion go up to 1 gigabyte . for bias , obviously , we look at the mean. for the raw series we want an estimate. for the corrected series , depending on how good the corrector is , we may want an estimate or we may want a test . we normally expect there to be bias in the raw series. so the tests which are applied to the raw series , apart for the test for bias , must be insensitive to bias. so our random number test suite much provide versions of these tests which are insensitive to bias . for drift , i suggest dividing the series into a small number of blocks and testing that the mean is the same in each block. a number of other tests for drift are also possible . for short term auto-correlation , obviously work out the auto-correlations. i look at the first auto-correlation by itself and the auto-correlations 2 to 7 as a combined test . for other short term dependencies i use the chi-square goodness of fit test for the distribution of words consisting each of 4 , 8 and 16 bits after partitioning the sample into 4 , 8 or 16 bit words. the name monkey test was introduced by marsaglia and is like the chi-squared test but uses all 4 , 8 or 16 bit sequences of bits rather than just disjoint ones. for most alternatives it is rather more sensitive than the chi-squared tests. in both cases , the 16 bit version does not have a lot of sensitivity and is probably not a lot of use . for frequency components , obviously use the spectrum or periodogram . i think i need to do more investigation on the detection of 1/ f noise. the one example i have of semi-conductor noise that really showed this effect had a power of around 0.75. i.e. i was seeing ( 1/ f ) 0.75 noise. for this type of noise just looking at the first auto-correlation does a reasonable job. see my 1987 paper on hurst effect . for other non-whiteness , look at the spectrum , of course . usually , when one looks at the results of the tests , these results are not very exciting. the generator passes the test and there is nothing more to report. here is an exception that shows the spectrum of a noise generator badly contaminated by a frequency component. this is from the german generator on marsaglia  s cd-rom. of course , marsaglia  s diehard tests said the generator was hopeless but didn  t say what was wrong . this is the spectrum from the canadian generator on the cd-rom and shows what a spectrum should look like. when i was first doing these analyses , i would just look at a spectrum like this and say it doesn  t go over the bounds very often so it is ok. i didn  t get away with this for long and have had to provide an explicit significance test . defect test relevance bad spots chi-squared on blocks both back door ? ? ? both unknown and other bit-within-word means both variance of run length corrected cr/lf test both maurer corrected sparse monkey corrected now looking at some more of the possible defects . i don  t think there is a good test for bad spots. i divide the series into blocks of around 1000 bytes and do an 8 bit chi-squared goodness of fit on each block then sum the results. this is probably more sensitive than the simple chi-squared test for detecting bad spots that might upset just a few of the blocks. but i doubt whether it is very sensitive . i doubt whether there are any good tests for back-doors although i would be very surprised if one existed in a commercial product. it is unlikely that a generator with a back-door would exhibit bias or simple a auto-correlation structure. with the generators used by the lotteries commission i simply increased the sampling speed and watched the auto-correlation increase. this , together with the observed bias , suggested very strongly that it was a genuine hardware random number generator and not a pseudo-random number generator in a fancy box . i also include a number of general purpose tests for attempting to detect defects we hadn  t thought of or are relevant only to specific generators . if the generator assembles the bits into 8 bit bytes or 32 bit words it is a good idea to check the means of the individual bits to check that , for example , the bits at the ends of the byte or word are n't being lost. i originally included this test because the information i needed for it arose out of another test. but it does seem worth including because the electronics can lose bits at the beginning or end of a word . a run is a sequence of just 0s or a sequence of just 1s. there is not much point in looking at average run length since this provides no new information beyond that already available in the mean and lag 1 auto-correlations. but the variance of the run length does contain new information , so one can look at this as general purpose test with no specific alternative in mind. a run length test , but not this one , is used in the fips 140-1 ( section 4.11.1 ) test so i wanted a runs test in my set of tests. i have included the maurer test because it is often recommended. it is not at all sensitive and , in my experience , when it is beeping the other tests are screaming . the sparse monkey test is an adaptation of marsaglia 's dna test , but looking at long sequences ( eg 27 bits ) of single bits and seeing if there are any sequences that occur much too frequently. i doubt that it is much use , but it might trap a poorly hidden back door . the only test i am going to talk about in detail is the carriage return/line feed one . ascii copy from unix to pc binary representation of 10 two bytes 13,10 ( e.g. marsaglia  s canadian and german files ) ascii copy from pc to unix byte pair 13 , 10 single byte 10 same effect if you do an ascii write with a binary file on a pc if you copy a file from a unix machine to a windows pc in ascii mode the binary representations of 10 get converted to two bytes , a 13 followed by a 10. there is a corresponding error if you copy in the opposite direction. this error happens so often in real life that i now include a test for this error in my set of tests. it happened to marsaglia  s samples for the canadian and german random numbers and it is why he so strongly rejected the canadian one. there is a slight bias in the canadian one so it does fail one of the diehard tests after you fix the copying error. but the failure is not spectacular . there is a related problem with the convention used by the computer for the order in which the bits or bytes are stored in a word. pcs follow the little-endian convention and some mainframes follow the big-endian convention. copying between machines that follow different conventions might rearrange the bytes and so reduce the effectiveness of the correlation and monkey tests , in particular. there can also be a problem if you are using a program written for one convention on a system designed for the other convention - e.g. marsaglia 's diehard tests used on a pc . must always be prepared to be surprised. but  there is no obvious mechanism in the rng for producing long-term dependence apart from simple trends and cycles . only a few general purpose tests for this kind of effect . emphasis is on testing for and measuring the kind of defects that one would expect to see . defects are likely to be more obvious and simpler before the corrector rather than after so emphasis is on testing the raw bits whenever possible . in the testing procedures we have to be prepared for the unexpected , which is why i have included some general tests . however , it is hard to see a mechanism for producing complex long-term dependence patterns apart from trends and cycles. so the testing concentrates on bias and short-term dependence. long-term dependence is really hard to detect because there are so many possibilities and tests that look for a wide range of possibilities will not be at all sensitive . defects are likely to be more obvious and simpler before the corrector than after , so if at all possible , we want to do the majority of the testing on the raw sample . classify tests as very important  report 1 % significance important  report 0.1 % significance less important  report 0.01 % significance in general , print out -log 10 ( significance probability ) the test suite involves a large number of tests , especially as there are several versions of some of them. if we do the complete set we are very likely to get some significances even from a perfect generator. i have classified the tests into three groups. for the very important i report any that are significant at the 1 % level. the tests classified as very important depend on whether one is looking at the raw or corrected dataset. however they include the bias , autocorrelation , the 4 and 8 bit monkey tests. the less important ones include the general purpose tests . with this classification the number of false alarms is reasonably manageable. if a test does report a significant result one can run another sample and do the test again . application accuracy true or pseudo speed encryption medium true or mixed low for use , medium for test encryption - otp medium true medium to high lottery high true low for use , medium for test experimental design medium to high ? low for use , medium for test statistical simulation compromise ? high i want to look briefly at the accuracy and speed requirements for the various applications. for different applications one has different accuracy requirements so the final application is also relevant to the testing process . for keyword generation in encryption one can estimate how close to uniform random the generator needs to be and the answer is that minor variations from uniform random don  t greatly reduce the security  this is not surprising since one uses only around 100 bits in keyword ( or maybe a few 1000 in some applications ) . similarly , speed is not a problem for generating the keys since only a small number of bits are involved. generators that run at around , say , 1000 bytes per second or more will be sufficiently fast. but for testing we do want moderately high speed since we probably want to test on a 10 megabyte set of numbers and don  t want to wait all day for them. around 10,000 bytes per second is fine for this. ideally you need a hardware random number generator , but there are schemes involving a pseudo-random number generator plus some random input that seem reasonably secure . there is a particular type of encryption known as one-time-pad. for this , speed may be important , since one will want to generate quite a number of megabytes. this has to be in a secure situation and so we wo n't want the generation to take too long. i do n't know if one-time-pad is used in practice these days. for one-time-pad only moderate accuracy is required but a hardware rng must be used . for lotteries , accuracy is important. for keno we might require that the probability of each possible draw is within 10 % of the theoretical value. one can use this to work out the required accuracy of the hardware generator. speed is not important for the actual draw , but it is important for testing , not only the generator itself but also for testing the processing software between points e and f in the flow diagram. i suspect the requirements for experimental design are similar , although it is peculiar circumstances that require the use of a hardware random number generator . for statistical simulations the big requirement is speed. i include simulation of statistical tests , simulations of complex models , markov chain monte carlo etc in this section. unless you need a lot of numbers you may as well use a pseudo-random number generator. so if you are going to use genuine random numbers you will need a lot of them. i suggest compromising on accuracy and leave out the corrector if you can , run the generator faster than recommended and rely on the combining with a pseudo-random number generator to remove the bias and auto-correlation. but there is no need to generate the numbers as you need them. at 10,000 bytes per second you can get around 1 gigabyte per day which can be stored on disk and accessed as required. i think a reasonable approach would be to use a good pseudo-random number generator for simulation most of the time but occasionally cross-check results with a simulation based on a hardware generator . return to robertnz.net random number generator page return to sra papers page
