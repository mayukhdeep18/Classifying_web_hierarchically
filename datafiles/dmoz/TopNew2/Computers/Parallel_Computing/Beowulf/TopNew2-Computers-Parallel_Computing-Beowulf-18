this page describes basic configuration and most important commands used on the cobalt ( computers on benches all linked together ) cluster , and covers the following topics : example scripts for several computers . overview of cobalt hardware overview of software installed on cobalt quantum chemical software on cobalt the cobalt gallery the cobalt poster cobalt : presentation at the nrc cluster workshop , november 17 , 2000 user ids and home directories on cobalt file systems , scratch space and remote access queuing system cobalt queues parallel jobs on cobalt printing from cobalt cd backups personal home pages on cobalt things which do not work recent changes due to privacy , security , and licensing concerns , some of the links in this document may be inaccessible for systems outside of the chemistry department of the university of calgary. parts of this page may be displayed incorrectly by browsers without table support , such as lynx . hardware cluster machines have names from cobalt1 to cobalt94 , which can be abbreviated to co1 , etc. all nodes belong the chemistry department 's domain ( .chem.ucalgary.ca ) , so that the fully qualified internet name for cobalt1 is cobalt1.chem.ucalgary.ca . all nodes are 500mhz 21164 digital alpha systems , each with 4gbyte local hard drive and a 100mbit ethernet card installed. regular nodes do not have operating system installed on them ( this is provided by the central server ) , so that the disk space on each node is split between memory paging space , user areas , and scratch space. here is the list of the other goodies on each of the nodes : node ( s ) total memory ( mbytes ) scratch ( gbytes ) 9-23,92-94 18 64 3.4 25-40,67-91 41 128 3.1 3 , 7 2 192 3.1 1 , 4 , 5 , 6 , 8 5 256 3.1 2 1 384 3.1 41-66 26 512 2.5 cobalt24 is special - it is the cluster 's central computer , serving the operating system and necessary databases to the rest of the cluster . it is outfitted with 128mbytes of memory and 31gbytes of disk space organized in mirrored disk arrays for increased reliability and performance . it uses two 100mbit ethernet cards to pump the data to other nodes . you can not run any jobs on cobalt24 , and should not use it for interactive work - it is quite busy as it is . all cobalt nodes are connected by four 3com fast ethernet switches , which together provide a gigabit cluster backbone. they also have a very pretty set of flashing lights. the cluster is controlled by means of a vt105 serial terminal , which adds to the rarity value of the whole hardware setup. cd recorder device ( yamaha crw4260t ) is physically located in room 431a in science b , and is available for backups. the final piece of hardware constituting the cobalt cluster is an industrial-grade air cooling unit , which prevents the whole thing from going up in flames . cobalt is connected to the outside world by a single 100mbit ethernet line . guided tours of the cobalt machine room are available on request. in the meantime , please visit the cobalt gallery . software cobalt runs compaq tru64 unix ( formerly digital unix , formerly osf/1 ) version 4.0d. all cobalt nodes have access to : development environment , including dbx and ladebug debuggers , atom ( profiling and analysis tool ) , cvs , rcs and sccs ( source code control systems ) , etc ; digital c compiler ( two versions , see man cc ) ; digital fortran compiler ( both f77 and f90 ) ; kap optimizing preprocessors for fortran-77 and fortran-90 ; digital 's extended mathematical library ( dxml ) , as well as many other packages available under the campus software licence grant program. you can find manuals ( plain text ) and release notes at /usr/local/doc . ( due to licensing restrictions , hosts outside chemistry department wo n't be able to access this link ) . a lot of free software is available in the /freeware directory ( which contains a copy of the freeware cd supplied by digital ) , and in /usr/local/ directory tree. some of the more useful freeware packages installed there are tetex and netscape . three versions of netscape are installed on cobalt : netscape navigator 3.04 gold ( installed as netscape ) , netscape communicator 4.07 international version ( netscape4 ) , and netscape communicator 4.07 us version ( netscape4us ) . due to the u.s. export restrictions , only u.s. and canadian nationals or permanent residents are allowed to use netscape4us . pvm parallel programming library and manual pages are installed in /usr/people/pvm . dqs distributed queuing system is installed in /usr/people/dqs . gaussian-94 is installed in /usr/people/wagstaff/g94 . adf and paw density functional codes are installed in /usr/people/program . mpich implementation of the mpi message passing standard in installed in /usr/people/mpich . environment variables necessary to access most packages ( most notably path , manpath , and ld_library_path , but also others ) are initialized in /etc/csh.login ( for csh and tcsh users ) or /etc/profile ( for sh/ksh users ) . quantum chemical software adf ( amsterdam density functional ) several versions of adf are installed on the cobalt cluster. not all features are available in all versions of adf , so that depending on your project you may have to use different versions. here is a short summary of the adf versions available on cobalt . version binary location source code location 1.1.5 n/a ~ program/adf/adf115/ 2.2. ? n/a ~ program/adf/src/fix2.2/ 2.3.0 n/a ~ program/adf/src/ref2.3/ 2.3.3 ~ program/adf/bin/adf ~ program/adf/src/fix2.3/ nmr for 2.3.3 ~ program/adf/bin/adf.nmr + ~ program/adf/bin/nmr ~ program/adf/src/nmr2.3/ 2.3.3 + qm/mm ~ program/adf/bin/adf.qmmm n/a ? 2.3.3 + cosmo ~ program/adf/bin/adf.solv n/a ? 2.3.3 + qm/mm + nmr + parallel ~ program/adf/bin/adf.parallel ~ patchkov/adf 98 + cosmo ~ program/adf/bin/adf98 n/a ? 99.02 ~ adf99/adf/bin/adf ~ adf99/1999.02/ 2000.02 ~ adf2000/adf.02/bin/adf ~ adf2000/2000.02/ 2002.02 ~ adf2002/adf.02/bin/adf ~ adf2002/2002.02/ in order to run any of the 2.3.3 , 98 , 1999 , and higher , binaries , you should set adfhome and adfbin environment variables respectively to ~ program/adf and ~ program/adf/bin . in c shell , the necessary commands will be : setenv adfhome ~ program/adf setenv adfbin ~ program/adf/bin doing this is particularly important if you are going to run adf in parallel . adf version 99 and higher requires several environment variables to be set in addition to adfhome and adfbin . the easiest way to achieve this is to execute a standard initialization script before starting adf : in c shell : source ~ adf99/profile.csh in bourne or korn shell : . /usr/people/adf99/profile.sh the 2000 and higher versions have similar scripts in their respective home directories . please refer to the queueing system and parallel jobs sections for the instructions for submitting your adf jobs to the queueing system . the key features supported by each of the pre-compiled adf binaries are summarized in the following table. this list is by no means complete ; please consult the reference manual and the program source code for the corresponding version for the complete list of supported features. the adf documentation is available at the website of scientific computing and modeling ( scm ) . version parallel 1 qm/mm 2 cosmo 3 nmr-sw 4 nmr-gs 5 2.3.3 - - - - - 2.3.3/nmr - - - yes - 2.3.3/qmmm - yes - - - 2.3.3/cosmo - - yes - - 2.3.3/parallel yes yes - - yes 98/cosmo - - yes - - 99.02 yes - yes yes - 2000.02 6 yes yes yes yes - 2002.02 7,8 yes yes yes yes yes 1 parallel execution using pvm 2 qm/mm code of tom woo 3 cosmo code of cory pye 4 scalar , spin-orbit , and zora nmr , stand-along program of steven wolff 5 scalar nmr and esr , code of georg schreckenbach 6 nasty bug for z-matrix optimizations. fixed in 2002.02 7 includes codes for analytic second derivatives and spin-spin couplings , and gs-nmr/esr code as standalone version 8 naming convention for basis set directories has changed. it is now sz , dz , dzp , tzp , tz2p instead of i , ii , iii , iv , v. see the adf manual for details . once you picked up the version of adf you are going to use , stick with it : binary files ( including tape21 and tape12 files ) generally are not compatible between adf releases . paw ( projector-augmented plane waves ) paw has its own home page on cobalt , please go there . user ids and home directories all home directories have permanent limit of 300 megabytes on the disk space usage. for short periods of time ( e.g. program builds ) , somewhat more disk space may be used. the excess has to be cleared within a week - otherwise , write permissions will be withdrawn automagically. if you are over your home quota ( i.e. if ' df -k . ' in your home directory shown 0 blocks available ) , you can check the remaining time before write permissions will be withdrawn by logging to the node containing your home directory and issuing command ' showfsets local people ' . using the node containing your home directory for interactive logins will improve response time and reduce load on the cluster interconnect . try to use " your " node for most of interactive work. you can also use cobalt1 to cobalt8 for interactive work which requires more memory than is available on your home node , but please try to avoid doing interactive work on cobalt nodes with numbers above 40 - these nodes may be used for running parallel jobs ( see below ) . any interactive load on these nodes may disrupt load balancing . running production jobs interactively is strongly discouraged. you can use " your " node for interactive debugging , or running small test jobs . everything else should go to the queue. in cases where it is absolutely necessary to run large jobs interactively , individual arrangements must be made with cluster administrator before job is started . all home directories are backed up nightly. due to the space limitations of backup media , about 1gbyte is available for each cobalt user. within this limit , up to about five most recent backups will be stored , and can be used for recovery of accidentally removed or damaged files. long-term backups are the sole responsibility of the users , and can be made using either cd recorder connected to the cobalt cluster , or tape backup services provided by acs . additional personal disk space excluded from nightly backups is available for special projects on request . home directories are physically distributed over cobalt nodes . here is the list of nodes with home directories which was accurate at the time this document was prepared. ( due to privacy and security concerns , this link is available only within chemistry department ) . you can always find the location of your home directory by saying ' ypmatch ` whoami ` auto.home ' while logged on any cobalt node . file systems , scratch space and remote access . on each of the cluster machines , /local/scratch contains local scratch space. the amount of the scratch space which is ( nominally ) available on each node is given above . somewhat more disk space may be available in practice. however , the remaining space is shared with local system temporary directories ( /tmp and /var/tmp ) and home directories , and should not be counted upon . please remove files from the scratch area as soon as you do not need them anymore , so as to make more space available for other users . all files older that two weeks will be automatically deleted from /local/scratch at midnight . all scratch and and home directories located on cluster hosts are accessible throughout the cluster. all home directories for local cluster users are accessible under /usr/people/user_name . scratch directory on a cluster machine host_name is accessible as /usr/remote/host_name/local/scratch . for example , for cobalt7 's scratch : /usr/remote/cobalt7/local/scratch . this name also works on cobalt7 itself . sharing of file systems throughout the cluster is implemented with automount service , so that directories are mounted when needed and unmounted if not used for more than a few minutes. as a consequence , taking a directory listing of /usr/people for example , may not list some , or all of the possible home directories. however , referring to any of the remote files by the explicit name ( e.g. , taking a directory listing of /usr/people/patchkov ) will work correctly. the same applies to all /usr/remote entries . with some shells ( sh , csh ) making an automounted directory your current directory will result in " strange " directory names being reported by pwd command. for example , doing " cd /usr/people/patchkov ; pwd " on any of the cluster machines except for cobalt18 may report /tmp_mnt/cobalt18/usr/people/patchkov . please do not use these names in any of your scripts , and not not rely on them. unlike the /usr/people and /usr/remote names , these are internal to automounter , and do not possess the magic necessary to make your home directory appear on a remote machine . in korn shell , pwd works as expected. tcsh ( available at /usr/local/bin ) can be configured either way , please read the corresponding manual page . cobalt nodes trust each other. all r-commands ( rlogin , rsh , rcp ) will let you access any cobalt node from any other cobalt node without a password . macintosh users on the chemistry department network can access their cobalt home directories using appleshare. to do this , pick cobaltcluster is the chooser , and provide your cobalt password when asked for the password . this service is implemented using cap ( columbia appletalk protocol ) , see corresponding manual page ( ' man cap ' ) . queuing system cobalt cluster is controlled by the distributed queuing system ( dqs ) version 3 . some dqs documentation is available in the form of manual pages for most important dqs commands ( qdel , qstat , and qsub ) , as well as in html form . either way , documentation is often misleading or plain wrong , so here is a minimal synopsis of the most important commands : qstat is used to examine status of jobs and nodes controlled by the queuing system. qstat without arguments will list all jobs known to the queueing system. ' qstat -u user_name ' will list jobs submitted by any particular user. ' qstat -f ' will include status of nodes in the output . qjobs is a simple perl script calling qstat and formatting the same information in the ( hopefully ) more useful form. without arguments qjobs will show only your jobs. qjobs -all will show all jobs know to the queuing system , while qjobs -u user_name will show jobs submited by user user_name . qdel is used to remove jobs ( either running or waiting in the queue ) . the only argument accepted by qdel is job identification number . qsub is used to submit jobs to the queuing system. in the simplest form , the only argument supplied to qsub is the name of a shell script which should be executed by the queuing system. a dqs batch job may look like this : # ! /usr/local/bin/tcsh # $ -n myfirstjob # $ -s /usr/local/bin/tcsh # $ -j y # $ -o outputofmyfirstjob.out # $ -l mem.ge.32.and.disk.ge.100.and.express cd $ tmpdir ~ program/adf/bin/adf98 < ~ /inputformyfirstjob.in every batch job consists of two sections : dqs header ( lines beginning with ' # $ ' at the top of the script and actual commands constituting the job. the header must be at the very top of the job file - any shell commands or comments apart from the magic ' # ! ' line will signal the end of the dqs header . the example above uses most of the normally encountered dqs directives , namely : -n sets the name under which the job will appear in the qstat output -s specifies shell which should be used to start the script. note that the magic ' # ! ' line , which would normally be used to specify shell in a unix script is in fact ignored by dqs -j line instructs dqs to combine standard error and standard output in the same file -o specifies location of the file which will receive standard output produced by the job. unless this name begins with the slash / , it will be interpreted relative to your home directory -l specifies resources needed by the job. currently , dqs on cobalt knows about the following resources : mem is the amount of memory available on each node disk is the amount of scratch space on each node paw is true if node can run paw jobs express is true if node can run express jobs parallel is true if node can run parallel jobs a special resource ' qty ' specifies number of nodes for parallel jobs , e.g. ' qty.eq.4 ' will ask for four nodes . each node should still satisfy all other resource constraints. parallel jobs are described in more detail below . many ( but not all ) resource specifiers can be combined ( see queues below to see which combinations are allowed ) . at the very least , your job should specify the amount of memory ( mem resource ) it will need . when dqs attempts to start a job , the resource record specified in the header is matched against the attributes of each available queue . if all the attributes match , the job is started. in particular , the example above requests any queue with 32 or more megabytes of memory , 100 or more megabytes of scratch space , and capable of running express jobs . when job is started , dqs will create a temporary directory in the scratch area ( /local/scratch ) on the node running the job , and set tmpdir to point to this directory. each job is started with current working directory set to your home directory. if your job needs to create substantial temporary files , you should use scratch directory provided by dqs for that purpose . dqs will take care of removing any files remaining in the area pointed to by tmpdir when your job terminates . some of the programs used on cobalt provide their own scripts for submitting dqs jobs , in which case you can simply use the ready script instead of writing your own. scripts used by paw are described in the paw documentation . queues cobalt provides the following queues : queue type cpus max time max memory max disk required keywords serial adf 58 4 weeks 110mb 3.4gb none ( default ) express adf 15 30 minutes 45mb 3.4gb express parallel adf 28 ( 7x4 ) 4 weeks 110mb 3.1gb parallel serial paw 10 4 weeks 480mb 3.1gb paw express paw 8 2 hours 360mb 3.1gb express , paw parallel paw 6 ( 2x3 ) no limit 480mb 2.5gb parallel , paw not that the number of queues of each type , as well as the resource limits , are adjusted from time to time depending on the workload. therefore , the numbers in this table should be treated as a guideline only . serial adf queue is the default queue , which will be used if no resource keywords were specified in the dqs job header. use of all other queues requires addition of the keywords given above to the resource record. individual nodes assigned to a given queue may have less memory or disk space than the queue 's maximum value given above. at the very least , you should always specify the amount of memory your job will need for execution . additional restrictions exist for the use of express and parallel queues. you are not allowed to run more than one express job at any given time. parallel adf jobs can use either four or eight nodes . other node counts are not allowed for load balancing reasons. parallel paw jobs can use three nodes. neither smaller nor larger node counts are permitted. it is absolutely forbidden to run parallel jobs in regular queues . these restrictions are not enforced by any mechanical means , so it is your responsibility to ensure they are not violated. repeated offenders will be barred access to the parallel and express queues . the queuing system places an overall limit on the number of cpus all your jobs together may use. the limit is set by the system administrator , and is adjusted periodically to ensure fair access to the cobalt queues . for example , if the overall limit is set to eight , and you have an 8-cpu parallel adf job running on cobalt , no new jobs submitted by you will start until the parallel job terminates or the per-user job limit is increased by the system administrator. if ' qstat -u ` whoami ` ' shows all your jobs waiting in the queue with the maxjob attribute , this is it . if you feel what neither one of the default queues satisfies your requirements , please do not try to circumvent the queueing system. come and talk to me , and we 'll try to find a solution . parallel jobs cobalt is a distributed-memory system , and requires explicit message passing programming in order to run jobs utilizing more than one cpu. two message passing libraries are currently supported on cobalt : pvm and mpi . adf was parallelized with the pvm library , while paw utilizes mpi . running a parallel adf job requires a slight modification of the sample job script above : # ! /usr/local/bin/tcsh # $ -n paralleljob # $ -s /usr/local/bin/tcsh # $ -j y # $ -o outputofmyparalleljob.out # $ -l mem.ge.32.and.disk.ge.100.and.parallel.and.qty.eq.4 # $ -par pvm cd $ tmpdir setenv adfhome ~ program/adf setenv adfbin $ adfhome/bin $ adfbin/start -n 50 adf.parallel < ~ /inputformyparalleljob.in or , if you prefer to run your parallel job using adf 99 ( just do not forget that you ca n't mix tape21 's and tape12 's between 2.3.3 and 99 - and you will also get slightly different numbers out of them ) , you can use this script : # ! /usr/local/bin/tcsh # $ -n paralleljob # $ -s /usr/local/bin/tcsh # $ -j y # $ -o outputofmyparalleljob.out # $ -l mem.ge.45.and.disk.ge.100.and.parallel.and.qty.eq.4 # $ -par pvm cd $ tmpdir source /usr/people/adf99/profile.csh $ adfbin/adf -n 4 < ~ /inputformyparalleljob.in the three important changes are : parallel and qty.eq.4 on the resources line ; an additional line identifying the job as a pvm application ( -par pvm ) ; finally , adf is now started with the start script , and , for adf 2.3.3 a special parallel version of adf is used . not all calculations can be done in parallel with adf.parallel at this point. it will run single-point scf and geometry optimizations in parallel , and should be able to execute hybrid qm/mm jobs. adf 99 should be able to run most of the jobs in parallel . the second application parallelized for cobalt is paw. ; instructions describing parallel execution of paw are given elsewhere . printing several printers are accessible from cobalt . obsolete : large text files ( program listings , etc ) should be printed out on < a href = " http : //www.ucalgary.ca/ucs/ " > acs < /a > duplex printer , which is selected by both ' < tt > lp < /tt > ' and ' < tt > lpr < /tt > ' by default. use of acs printers requires a valid acs aix account , along with an .rhosts file listing cobalt1.chem.ucalgary.ca to cobalt94.chem.ucalgary.ca in the acs home directory. printouts will come out in the basement of the math sciences building . chemistry department 's black and white postscript printer ( in room sb 229 ) is accessible as ' chem_ps ' printer. the colour postscript wax printer ( at the same location ) is accessible as ' wax_ps_color '. finally , a black and white postscript printer in sb 433 is known to cobalt as ' jana_ps '. in order to access any of these printers with lp or lpr use : lp -d chem_ps a_postscript_file.ps or : lpr -pchem_ps a_postscript_file.ps you can also set any of the printers as your default printer by doing : setenv printer chem_ps # in csh/tcsh or printer = chem_ps ; export printer # in sh/ksh all three postscript printers will only accept postscript input. if you need to print a text file , you will have to convert it to postscript first , e.g. : once you send the output to any of the printers , it is not possible to cancel it. be careful with what and where you print. it is not advisable to print large outputs on the jana_ps printer : people working in sb 433 get mightily annoyed with printer noise and having to replace the paper all the time. it is also a bad idea to print black and white output on wax_ps_color , which uses expensive paper and is much slower than chem_ps . both printers in sb 229 have a duplex option installed ( for double sided printing ) but it can be tricky to enable this option when printing from cobalt. for the wax printer , using the duplex_wax_ps_color print queue should do the job. for chem_ps you have the option to login to praseodymium.chem.ucalgary.ca and print the file from there with lpr my_file.ps . the print queue on this machine has been successfully set up to support the chem_ps printer 's duplex option. please note , however , that this printer frequently forgets that it has such an option and needs to be switched off and on in order to enable duplex printing. after having informed you about the hazards of trying to print double sided on this printer i should also note that the outcome is of really good quality and that it prints very fast . cd backups cobalt 's cd recorder ( yamaha crw4260t ) is physically located in sb 431a and is attached to zinc10 , which runs linux . 4260 supports recording ordinary cdr blanks at 4x speed ( 600kb/sec for data ) and ( the " silver " ) cdrw blanks at 2x speed ( 300kb/sec ) . recorded cds can be mounted either on zinc10 or , for extended use , directly on 'your ' cobalt node . alternatively , you can use the cd writer installed in the machine praseodymium ( pr ) , also located in sb 431a . you are required to bring your own blank cds for the recording session , as no blank cd are stocked in sb 431a. blank cds are available from the microstore ( in the basement of math sci building ) or from the new media centre ( in the basement of mackimmie library block ) . please contact the system administrator before using the cd recorder for the very first time . zinc10 and praseodymium will accept your cobalt password , and will automatically provide access to your home directory and all directories in /usr/people . in order to access scratch directories from zinc10/pr , you will have to use file names in the format : /usr/remote/cobaltnn.local.scratch/ . this is different from the cobalt proper , where you have to use slashes instead of dots in the pathname . macintosh users can also prepare data for cd session in the appleshare volume 'cdr scratch volume ' available from appleshare node cobaltcluster . this volume will appear as the directory ' /macvolume ' on zinc10 . if you use this volume to prepare your cds , please clean up after yourself once you are finished with your cd . the easiest way to record your data on a cd is to use the ' record-cd ' script which is located in /usr/local/bin directory on zinc10 and pr. this script takes a single argument - name of the directory containing files to be backed up , and creates a hybrid cd containing iso 9660 file system with both rockridge and microsoft joliet extensions. the resulting cd is readable on most unix systems as well as on microsoft windows machines . if your directory structure contains extended finder information ( i.e . if you created it on the 'cdr scratch volume ' or on a mac volume in your home directory - see man aufs for instructions on creating a mac volume on cobalt ) , ' record-cd ' will recognize extended attributes and create a cd containing mac hfs volume in addition to the iso , rockridge , and joliet file systems . in either case , you will be given an opportunity to examine the contents of your cd image before recording it. you can also record multiple copies of your cd using record-cd script . if you are not satisfied with the features provided by record-cd , you can always create your cd using mastering and recording utilities directly . the following cd mastering software are installed on zinc10 and pr in /usr/local/bin , with the corresponding manual pages in /usr/local/man hierarchy : mkisofs command line utility for mastering iso9660 , rockridge ( unix ) and joliet ( windows ) cds . a lot of interesting links on cd formats was collected by j ö rg schilling . both rockridge and joliet can be on at the same time. mkisofs also supports mastering multi session cds . note however that cobalt nodes are unable to read multi session cds due to a bug in cd firmware mkhybrid command line utility for mastering macintosh cds. it also supports rockridge and joliet , but not multi session. note : newer versions of mkisofs contain this functionality. on pr this program is therefore not installed . cdda2wav classic dae tool for unix. not tested and may not work cdrecord command line utility for recording cd images created with the programs above. cdrecord supports data and audio cds , and can create multi session cds. cdrw blanks are supported in the same way as cdr disks , but can be erased and reused . xcdroast gui interface for mkisofs/cdrecord. although it does not support multi session cds , mastering and recording single-session audio and data cds is quite easy with xcdroast. since this is a development version of the program , there are some minor glitches with it. in particular , if you use xcdroast from a remote machine , you will have to set display to the numerical ip address. host names wo n't work . personal home pages on cobalt project-related and personal home pages of cobalt users will be created on request. at the time this document was last updated , the following home pages were created ( but not necessarily populated by their owners ) : person or project external name internal name tom ziegler 's group http : //www.cobalt.chem.ucalgary.ca/group/ file : /usr/home_pages/group/ mary chan http : //www.cobalt.chem.ucalgary.ca/chan/ file : /usr/home_pages/chan/ cory c. pye http : //www.cobalt.chem.ucalgary.ca/cory/ file : /usr/home_pages/cory/ tom gilbert http : //www.cobalt.chem.ucalgary.ca/gilbert/ file : /usr/home_pages/gilbert/ yana khandogin http : //www.cobalt.chem.ucalgary.ca/khan/ file : /usr/home_pages/khan/ liqun deng http : //www.cobalt.chem.ucalgary.ca/ldeng/ file : /usr/home_pages/ldeng/ minserk cheong http : //www.cobalt.chem.ucalgary.ca/mcheong/ file : /usr/home_pages/mcheong/ artur michalak http : //www.cobalt.chem.ucalgary.ca/michalak/ file : /usr/home_pages/michalak/ the paw project http : //www.cobalt.chem.ucalgary.ca/paw/ file : /usr/home_pages/paw/ serguei patchkovskii http : //www.cobalt.chem.ucalgary.ca/ps/ file : /usr/home_pages/ps/ rochus schmid http : //www.cobalt.chem.ucalgary.ca/rschmid/ file : /usr/home_pages/rschmid/ torben rasmussen http : //www.cobalt.chem.ucalgary.ca/tra/ file : /usr/home_pages/tra/ kumar vanka http : //www.cobalt.chem.ucalgary.ca/vanka/ file : /usr/home_pages/vanka/ tom ziegler http : //www.cobalt.chem.ucalgary.ca/ziegler/ file : /usr/home_pages/ziegler/ petitjean laurent http : //www.cobalt.chem.ucalgary.ca/laurent/ file : /usr/home_pages/laurent/ christoph widauer http : //www.cobalt.chem.ucalgary.ca/widauer/ file : /usr/home_pages/widauer/ jochen autschbach http : //www.cobalt.chem.ucalgary.ca/jochen/ file : /usr/home_pages/jochen/ eva d. zurek http : //www.cobalt.chem.ucalgary.ca/edzurek/ file : /usr/home_pages/edzurek/ the internal name of each home page ( which always starts with file : ) is only accessible to browsers running on the cobalt cluster itself , but allows modification of the page contents. the external name only allows read-only access , but is valid at any internet location. the external name is what you want to give to your friends so that they can see your cool home page . the contents of your home page directory are entirely under your own control. however , you are expected to follow few simple rules , namely : please do not create symbolic links in this area pointing outside of the web area please try to limit your disk space use to a reasonable minimum please follow the uofc guidelines for the personal web pages please do not use cgi scripts in your home pages ( they wo n't work anyways ) use common sense , and ask me if you are not quite sure whether you are doing the right thing or not . most importantly , enjoy it ! things which do not work thanks to the goedel theorem and murphy law , there should be some . if you find any , please let me know. i am also open to suggestions on improving and extending this document . recent changes aug. 29 , 2002 added link to adf 2002 and home page of e.z. , printer info updated sept. 28 , 2001 added link to adf 2000 , other updates may 15 , 2001 added link to j.a. 's home page december 6 , 2000 added link to cobalt nrc presentation september 30 , 1999 documented installation of adf 1999 on cobalt may 20 , 1999 added a link to the cobalt poster march 24 , 1999 added section on personal home pages march 12 , 1999 added a link to the cobalt gallery march 1 , 1999 fixed links mangled by netscape communicator. all links should work now this document was initially prepared by serguei patchkovskii , who took care of the day-to-day operations of the cobalt until sept. 2001 and is a nice guy all round . jochen autschbach took care of cobalt , from sept. 2001 to sept. 2002. he is also a nice guy. another nice guy zhitao xu took care of cobalt from oct. 2002 to oct. 2003. since nov. 2003 , another guy , michael seth who might be nice we have n't decided yet , has been nursing cobalt along .
