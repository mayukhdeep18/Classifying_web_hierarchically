the hermes linux production cluster since sep. 6th 1996 hermes has got a new production farm for extracting the physics out of the events occuring inside the hermes detector . please feel free to read more about this interesting experiment on the hermes homepage . requirements the requirements for cpu power in an hep ( high energy physics ) experiment are high. to get an impression : per day an average data volume of 33gb is recorded which adds up to several tbs stored on robot tape drives per year. on the other hand results have to be published soon after the data has been recorded. as a precise knowledge of all detector parameters can be obtained only after several analysis iterations our goal is to run one iteration of one year 's data within less than 2 months . for comparison : the batch data processing of 1995 data ( 2.5 tbs ) was done on a 28 multiprocessor sgi challenge xl system. as the data volumes increased ( 1996 3.5tb , 1997 6.5tb ) it was decided to extend the computing power for hermes and move the data processing on a separate system. the interactive physics analysis will still be done on the sgi system . the experiments raw data is stored on fast tapes and must be read with a speed of up to 2mb/s in order not to block the tape drives unnecessary long. the i/o-bandwidth inside the production system does not have to be high. average rates of 200-800kb/s throughput have to be handled , only . choosing a new platform also has to take the available analysis software into account. all the software for hermes was developed on unix systems. as several different unix platforms were in use by the collaboration it was tried to maintain the code in a ( unix- ) system independent way. code is available in c , c + + , fortran77 , bash , perl and tk/tcl dialects . the cluster components shifting the batch production from the sgi multiprocessor system while keeping the production speed was achieved by installing a cluster of 10 pentiumpro200 systems running linux. ( later - see below - each pc was upgraded to a 2 cpu dual-ppro system ) . the data access to the tape silos is done via fast-ethernet ( 100basetx ) and an fddi-100basetx switch. recently the switch has been replaced by one of the pcs configured as an fddi/fast-ethernet router. each pc now is equipped with asus motherboard p65up5 with 2xppro 200 cpu ( 256k cache ) , 4x32mb edo ram ( 60ns ) , ncr scsi controller , 2x4gb ibm dfrs-34320 fastscsi2 disks , s3 trio graphics card ( only for installation ) , 3.5' ' floppy disk ( installation ) , smc etherpower 10/100 ( dec tulip chip ) fast ethernet card . the machines were bought at comptronic , a hamburg computer dealer. their excellent support justifies not only this link.. . currently - and this situation is unlikely to change in near future - hardware of this type offers a price/performance ratio of a factor 2 to 5 better than traditional workstation systems . the operating system there 's a variety of operating systems for pcs. the most popular ones certainly are ms-dos and other microsoft products ( mainly winnt ) , ibm 's os/2 and various unix dialects : sco , solaris , free-bsd and linux . a rather famous paper comparing different unix flavours for pcs is the lai-baker paper ( proceedings of usenix96 ) . the linux version taken for this comparison is still 1.2 , so be sure to read to the end of the paper where the development and progress issues for newer kernels ( 1.3 ) are mentioned . high energy physics software is strongly correlated with the cern software library which offers a huge amount of ( mostly ) f77 library calls. the cern libraries are available to only a few of the pc operating systems - so the choice becomes more easy : only winnt and linux are left . porting the hermes software ( 500.000 lines of code ) to new technologies like nt seems to be a challenging task. several similar tasks are currently under investigation and a realistic timescale for the case of hermes is in the area of 0.5 to 1 man years . in this respect the additional cost for the os , development software , extra network licences ( we need more than 10 internet connections per process ) is even rather small . so the choice was obvious : linux. a free unix operating system with additionally excellent performance features. porting involved the design of a f77 standardising filter ( f77reorder ) as our code used many non standard extensions to f77 , handled by most compilers but not by f2c or early g77 versions , the free fortran compilers for linux . with an effort of 2 man weeks the whole hermes source tree got translated and was brought into operation . the current status 16.sep.96 well the cluster is still young. currently it is busy with monte-carlo production ( computer simulated evens in the detector ) as data processing requires a new setup of the production daemon which is able to distribute tasks to the different cluster members. ( at this time the machines still were single cpu machines ) . 22.sep.96 on saturday and sunday ( 21-22.sept 96 ) a first test production of raw 96 data has successfully been run on the cluster. the processing was done at the expected speed of about a factor of 2.2 faster than on a single sgi challenge mips 4400 /200mhz cpu . 27.sep.96 the 3com905 fast ethernet cards got replaced by smc etherpower 10/100 cards which now offer full usage of the 100mbit/s bandwidth of fast ethernet . 15.nov.96 monte-carlo production has finished and new calibration data for 96 is available. we start the first test production on 100 runs of 96 . a central job and disk manager controls the job execution on the cluster via the hermes d ad slow-control scheme . click on this graph for the production control layout : job execution can be monitored using p in k clients : the results of the production look promising. together with code optimisation the speed gain with respect to the last production on our large smp machine is in the order of 3 per cpu . 27.nov.96 now that everything is shown to work reliably we have to 'sell ' the product : some transparencies ( 656kb ) from a talk given at cern about the linux production cluster . 20.dec.96 the data production is running after some final corrections to programs and detector calibration have been applied. however we experience problems with our fast-ethernet/fddi switch. after an average of 12 hours the fast-ethernet part of the switch stops execution under heave load on the fast-ethernet segment . 23.dec.96 we replace the faulty switch by one of the pcs which is now equipped with a dec dc21140 fddi adapter . ( thanks to dec for the driver development ! ) . now a stable operation seems to be possible : hermes members can run the above screen using the clusterstatus.pink command on our sgi . 12.feb.97 the first iteration of 96 data on the pc farm has finished in time . 28.mar.97 up to know the machines have been 100 % stable - no crashes since 23.dec.96. this results in an integrated uptime of 2.5 years for the cluster. the systems were fully loaded for 95 % of the time . 29.mar.97 never touch a running system - well.. . the smp upgrade arrives and the sorrow begins. first impression of the dual cpu systems is great. they perform at 199 % system usage and do in fact double the available resources. the price for the upgrade is small . but linux at this time contains a lot of dead-locks in its kernel and is not smp safe . various iterations of the tulip driver ( which was believed to be the faulty one in the beginning are tried ) . various kernel releases are installed and oopses recorded and sent to the developers . 20.may.97 finally we got a reasonably stable for our environment release of linux ( uptimes of about one week ) . a long time of booting ( uptimes of 12hours and shorter for 10 machines are quite annoying ,- ) is over . hermes runs a 4gips cluster for less than 60k $ . 30.jun.97 w.wander leaves hermes and heads towards mit/boston. andrei shevel takes over . 11.oct.97 a.shevel leaves back to st.petersburg. alexander kisselev becomes a new maintainer of the farm . nov-dec.97 extensive tests of the two recently released linux kernels ( 2.0.31 and 2.0.32 ) showed their excellent stability ( uptimes up to the manual reboot or another cpu overheating due to the broken fan ) . wolfgang wander , wwc @ ralph2.mit.edu , alexander kisselev created : fri sep 13 15 : 13 : 03 met dst 1996 hhmts start last modified : mon feb 23 11 : 47 : 18 1998 hhmts end
