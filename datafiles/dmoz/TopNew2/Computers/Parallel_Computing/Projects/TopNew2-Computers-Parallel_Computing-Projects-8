purdue 's adapter for parallel execution and rapid synchronization : the ttl_papers design h. g. dietz , t. m. chung , t. mattox , and t. muhammad school of electrical engineering purdue university west lafayette , in 47907-1285 hankd @ ecn.purdue.edu january 1995 abstract tightly-coupled parallel machines are being replaced with clusters of workstations connected by communication networks yielding relatively long latencies , but ever-higher bandwidth ( e.g. , ethernet , fddi , hippi , atm ) . however , it is very difficult to make parallel programs based on fine- grain aggregate operations execute efficiently using a network that is optimized for point-to-point block transfers . ttl_papers augments a cluster of pcs or workstations with the minimum hardware needed to provide very low latency barrier synchronization and aggregate communication. for example , unix user processes within a ttl_papers cluster can perform a barrier synchronization in 2.5 microseconds , including all software overhead. this is four orders of magnitude faster than using unix socket connections over an ethernet . this paper presents the ttl_papers principles of operation , implementation issues , low-level software interface , and measured performance of the basic operations . this work has been supported in part by onr grant no . n0001-91-j-4013 and nsf grant no. cda-9015696 . keywords parallel architecture , fine-grain parallelism , workstation clustering , barrier synchronization , aggregate communication . notice for html users the html version of this paper differs from the standard postscript version ( click here for the 157k compressed postscript version ) in several ways , including a variety of minor formatting changes. the standard postscript version was formatted by groff -mgs as a single file. in contrast , this html document was generated using a script that replaces figures with in-line gif images that are hypertext linked to full-resolution postscript versions ; the ( n k .ps.z ) at the upper right of each in-line image provides both the size of the compressed postscript file and the hypertext link. there are also a variety of hypertext links inserted for navigation within the document , including a hypertext index at the end of this document ( click here to jump to the hypertext index ) . 1. introduction ttl_papers is the ttl implementation of purdue 's adapter for parallel execution and rapid synchronization. although it provides sufficient support for barrier synchronization and aggregate communication to allow a cluster to efficiently execute fine-grain mimd , simd , or even vliw code , a four- processor ttl_papers unit uses just eight standard ttl chips. at first , it is very difficult to accept that such inexpensive and simple hardware can implement very effective synchronization and communication operations for parallel processing. this is because the traditional views of parallel and distributed processing rest on a set of basic assumptions that are incompatible with the concept : conventional wisdom suggests that the operating system should manage synchronization and communication , but even a simple context switch to an interrupt handler takes more time than ttl_papers takes to complete a typical synchronization or communication. all interactions with the ttl_papers hardware are i/o port accesses made directly from the user program ; there are no os modifications required and no os call overhead is incurred . communication operations are characterized primarily by latency ( total time to transmit one object ) and bandwidth ( the maximum number of objects transmitted per unit time ) . the hardware and software complexity of most interaction methods results in high latency ; high latency makes high bandwidth and large parallel grain size necessary. in contrast , ttl_papers is very simple and yields a correspondingly low latency. providing low latency allows ttl_papers to work well with relatively fine-grain parallelism , but it also means that relatively low bandwidth can suffice . a typical parallel computer is constructed by giving each processor a method of independently performing synchronization and communication operations with other processors ; in contrast , ttl_papers interactions between processors are performed as aggregate operations based on the global state of the parallel computation , much as in a simd machine. this simd-like model for interactions results in much simpler hardware and a substantial reduction in software overhead for most parallel programs ( as was observed in the pasm prototype [ sin87 ] ) . for example , message headers and dynamically-allocated message buffers are not needed for typical ttl_papers communications. it is also remarkably inexpensive for the ttl_papers hardware to test global state conditions such as any or all . thus , ttl_papers does not perform any magic ; it simply is based on a parallel computation model that naturally yields simpler hardware and lower latency. ttl_papers is not really a network at all , but a special-purpose parallel machine designed specifically to provide low-latency synchronization and communication , taking full advantage of the " loosely synchronous " execution models associated with fine-grain to moderate-grain parallelism . 1.1. synchronization the only synchronization mechanism implemented by ttl_papers is a special type of fine-grain barrier synchronization that facilitates compile-time scheduling of parallel operations [ dio92 ] [ dic94 ] . hardware barrier synchronization was first proposed in a paper by harry jordon [ jor78 ] , and has since become a popular mechanism for coordination of mimd ( mimd refers to multiple instruction stream , multiple data stream ; i.e. , each processor independently executes it own program , synchronizing and communicating with other processors whenever the parallel algorithm requires. ) parallel processes. a barrier synchronization is accomplished by processors executing a wait operation that does not terminate until sometime after all pes have signaled that they are wait ing. however , while building the 16 processor pasm ( partitionable simd mimd ) prototype in 1987 [ sin87 ] , we realized that the hardware enabling a collection of conventional processors to execute both mimd and instruction-level simd ( simd refers to single instruction stream , multiple data stream ; i.e. , a single processor with multiple function units organized so that the same operation can be performed simultaneously on multiple data values. ) programs was actually an extended type of barrier synchronization mechanism. generalizing this barrier synchronization mechanism resulted in several new classes of barrier synchronization architectures , as reported in [ okd90 ] [ okd90a ] . unlike other papers implementations , the ttl_papers barrier mechanism provides only a subset of these features ; however , these features are sufficient to enable conventional processors to efficiently implement fine-grain mimd , simd , and vliw ( vliw refers to very long instruction word ; i.e. , a generalization of simd that allows each function unit to perform a potentially different operation on its own data. ) execution [ cod94a ] . the ttl_papers barrier mechanism also provides an effective target for compile-time instruction-level code scheduling [ dio92 ] . 1.2. communication in addition to high-performance barrier synchronization , ttl_papers provides low-latency communication. this mechanism is not equivalent to a shared memory nor a conventional message-passing system , but has several advantages for loosely synchronous communication . basically , it trades asynchrony for lower latency and more powerful static routing abilities . as a side-effect of a barrier synchronization , each pe can place information into the ttl_papers unit and get information from whichever processor , or group of processors , is selected. thus , the sender only outputs data ; it is up to the receiver to know where to look for the data that the sender has made available to it. in ttl_papers , we further simplify the hardware by extending this concept so that all participating processors must agree on and contribute to the choice of what data each processor will have available. compared to conventional networks , this method allows less autonomy for each processor , but yields simpler hardware , better performance , and more powerful access to global state . the most basic ttl_papers communication operation is a multi-broadcast that sends to each processor a bit mask containing one bit from each processor. this is a very powerful mechanism for examining the global state of the parallel machine. an obvious application is for processors to " vote " on what they would like to do next. for example , it can be used to determine which processors are ready for the next phase of a computation or which processors would like to access a shared resource ( e.g. , to implement a balanced , conflict-free , schedule for access to an ethernet ) . however , because any communication pattern is a subset of multi-broadcast , it can also be used to implement general communication " routing. " in addition to one-bit multi-broadcast , some versions of papers provide the ability to get four bits of data from any processor in a single operation. using a unidirectional centronics printer port , four bits is the theoretical maximum number of data bits that can be obtained by one port input operation. ttl_papers implements four bit sends , and even more functionality , using simple nand logic to combine signals from the processors. operations directly supported include : since ttl_papers data is literally a four-bit wide nand of data sent across all processors , computing a four-bit global nand takes only one operation. likewise , nand can implement and and or functions by simply complementing the output or by complementing the inputs . to accomplish a four-bit broadcast from a single processor , all the other processors simply output 0xf -- four 1 bits . again , the result will be the complement of the data sent . to accomplish a one-bit multi-broadcast , each processor sends four bits of data such that processor i 's bit i is the data it wishes to send , and the other bits are all 1 values . for example , bit 2 will be 1 for processors 0 , 1 , and 3 ; thus , nanding these signals together will result in a signal that is the complement of bit 2 from processor 2 . in summary , ttl_papers provides almost no facility for autonomous communications , but does provide a very rich collection of aggregate communication operations based on global state . 1.3. interrupts to facilitate some level of asynchronous operation , ttl_papers provides a separate interrupt broadcast facility so that any processor can signal the others. such an interrupt does not really generate a hardware interrupt on each processor , rather , it sets a flag that each processor can read at an appropriate time. although such a check can be made at any time , the two most obvious times are : when a barrier wait has taken an unexpectedly long time . this would indicate that one or more processors might have generated an interrupt , freezing the barrier state so that other processors would know to check for an interrupt . when the os is invoked to perform a scheduling operation . thus , gang scheduling and other os functions can be implemented by having the os on each processor use the interrupt facility to coordinate across processors . the ttl_papers interrupt facility provides all the necessary logic for generating an interrupt and providing a special " interrupt acknowledge barrier. " 2. pc hardware although ttl_papers provides very low latency synchronization and communication , it is interfaced to pcs using only a standard parallel printer port and is implemented with a minimal amount of external hardware . this section details the pc hardware involved in use of ttl_papers . throughout the following description , we will distinguish between stand-alone pcs and pcs used as processors within a parallel machine by referring to the later as " pes " -- processing elements. the design presented here directly supports 4 pes ( pe0 , pe1 , pe2 , and pe3 ) , and we also detail how the design can be scaled to 8 , 16 , or 32 pes. in fact , no significant changes are needed to scale the design to thousands of processors ( the hardware structure that needs to scale is significantly simpler than the barrier and tree implemented in the cray t3d [ cra93 ] ) . 2.1. pe hardware interface no changes are required to make standard pc hardware into a ttl_papers pe. all that is needed is a standard parallel printer port and an appropriate cable. although some of the pcs on the market provide extended-functionality parallel ports that allow 8-bit bidirectional data connections , many pcs provide only an 8-bit data output connection. like the original papers , ttl_papers can be used with any pc ; it uses only the functions supported by a standard unidirectional parallel port . but if there is no parallel input port , how does ttl_papers get data into the pc ? the answer lies in the fact that the 8-bit data output port is accompanied by 5 bits of status input and 4 bits of open-collector input/output ( which are sometimes implemented as output only ) on two other ports associated with the 8-bit data output port. the way we use these lines , there are actually 12 bits of data output and 5 bits of data input . all versions of both ttl_papers and papers use all 5 available input lines. however , the various versions differ in how many and which of output signals are used. because the open-collector lines are generally not as well driven as the 8-bit data output lines and require access to a different port address , we generally use the open-collector lines only for signals that are modal , i.e. , that change relatively infrequently and can have large settling times . the version of ttl_papers discussed here uses 11 of the 12 available output lines , but only the 8-bit data output port is written in the process of normal interactions with the ttl_papers hardware ; the 3 other bits are used only for interrupt handling. table 1 summarizes the signal assignments for the pc parallel port as it is used for the ttl_papers interface . ( 5k .ps.z ) 2.2. pe port bit assignments although the parallel port hardware is not altered to work with ttl_papers , the parallel port lines are not used as they would be for driving a centronics-compatible printer . thus , it is necessary to replace the standard parallel port driver software with a driver designed to interact with ttl_papers. toward this end , it is critical to understand which port addresses , and bits within the port registers , correspond to each ttl_papers signal . there are three port registers associated with a pc parallel port. these registers have i/o addresses corresponding to the port base address ( henceforth , called p_portbase ) plus 0 , 1 , or 2. typically , p_portbase will be one of 0x378 , 0x278 , or 0x3bc , corresponding to ms-dos printer names lpt1 : , lpt2 : , and lpt3 : . as a general rule , most pcs use 0x378 for the built-in port , however , ibm pcs generally use 0x3bc. workstations based on processors other than the 386 , 486 , or pentium , e.g. , dec alphas , also generally use 0x3bc ; however , most of these processors map port i/o registers into memory addresses , so the port operations must be replaced with accesses to the memory locations that correspond to the specified port register i/o addresses . for example , the ibm powerpc specification places i/o address 0x3bc at physical memory location 0x800003bc . the bits within the register p_portbase + 0 are used to send ttl_papers both strobe and data values , as well as controlling a bi-color led ( red/green light emitting diode ) on the papers front panel. if a pc wants to mark itself as not participating in a group of barrier synchronizations , it should simply output 0xcf ; this corresponds to setting p_s1 , p_s0 , p_d3 , p_d2 , p_d1 , and p_d0 all equal to 1. notice that , if a pc is not connected to one of the ttl_papers cables , the ttl inputs will all float high , causing the missing pc to be harmlessly ignored in operations performed by the pcs still connected. in contrast , setting both p_s1 and p_s0 to 0 will ensure that all barrier operations halt . in normal operation , each ttl_papers operation is triggered by toggling the p_s1 and p_s0 lines between ( p_s1 = 1 , p_s0 = 0 ) and ( p_s1 = 0 , p_s0 = 1 ) ; this can be done by simply exclusive- oring the previous output byte with ( p_s1 | p_s0 ) . a pc sends data to ttl_papers by setting the output nybble bits appropriately and toggling the strobe lines as described above. performing this operation as two steps , first changing data bits then changing the strobe bits , can be done to increase the reliability of transmission. data lines should be given time to settle before a new ready signal is derived from the new strobe signals. changing strobes and data simultaneously results in a race condition in which the data bits have only about 20 nanoseconds " head start " -- a small enough margin for many systems to perform unreliably . the p_lr and p_lg lines are simply used to control a bi- color led to indicate the status of the pc relative to the currently executing parallel program. when ttl_papers is not in use , both bits should be set to 0 , yielding a dark led. when a parallel program is running , the led should be lighted green , which is accomplished by making p_lg = 1 and p_lr = 0. when a pc is waiting for a barrier , it should make its led red by setting p_lg = 0 and p_lr = 1. it is also possible to generate an orange status light by setting both p_lg and p_lr to 1 , however , this setting is used only rarely ( as a " special " status indication ) . the second port register , p_portbase + 1 , is used to receive data from ttl_papers. to enhance the portability of ttl_papers to somewhat non-standard parallel printer ports , only these five bits are used as input : four bits of data and one bit to act as a ready line. because 0x40 is the only bit that can be enabled to generate a true interrupt to the pc , earlier versions of papers made p_rdy use 0x40 so that the papers unit could generate a true hardware interrupt when a barrier synchronization had completed . however , this led to an inconvenient order for the bits of the input nybble , and we never found a good use for the true hardware interrupt ( because interrupt handlers have too much latency ) , so the current arrangement makes p_rdy use 0x80 . the new arrangement is superior not only because it keeps the bits of the input nybble contiguous , but also because the inversion of p_rdy is harmless , whereas the inversion of an input data line would require extra hardware or software to flip the inverted data bit ( e.g. , exclusive or with 0x80 ) . note also that p_rdy is a toggling ready signal. the original papers unit used software to " reset " the ready signal after each barrier synchronization had been achieved , thus requiring four port operations for each papers synchronization. by simply toggling p_rdy when each new barrier is achieved , ttl_papers can perform barrier synchronizations using just two port operations . the third port register , p_portbase + 2 , serves only one purpose for ttl_papers : parallel interrupt support . actually , for the reasons described earlier , ttl_papers never generates a " real " interrupt to a pc. however , parallel interrupts provide a mechanism for managing the use of the ttl_papers unit in a more sophisticated way , for example : providing a better ttl_papers " check-in " procedure , facilitating abnormal termination of parallel programs , implementing a user-level parallel file system , and even gang scheduling and parallel timesharing of the ttl_papers unit . to cause a parallel interrupt , a pc simply sets p_irq to 1 . however , other processors will not notice that a parallel interrupt is pending unless they explicitly check. this is done by changing p_sel to 1 , which causes the normal p_rdy ( described above ) to be replaced by an interrupt ready flag... until p_sel is again set to 0. thus , any pc can check for an interrupt at any time without interfering with the operation of other pcs ; for example , while delayed waiting for a barrier , it is essentially harmless to check for an interrupt. to encourage pcs to check for an interrupt , the interrupting pc can set its p_s1 and p_s0 bits to 0 ( see above ) , forcing barriers to be delayed. when all pcs set their p_nak to 0 , this simply acts to perform a special interrupt barrier. the extended interrupt functionality is implemented by sending an interrupt code as a side-effect of this special barrier . 3. ttl_papers hardware thus far , this document has focused on the way in which pc hardware interacts with ttl_papers. in this section , we briefly describe the hardware that implements ttl_papers itself. the ttl_papers design has been carefully minimized to use just 8 standard ttl 74ls-series parts and to fit on a single-layer circuit board ( shown in figure 1 ) . the result is a remarkably simple design that is inexpensive to build , yet fast to operate. the logic design for ttl_papers is logically ( and physically ) divided into three subsystems : the barrier and interrupt mechanism , the aggregate communication logic , and the led display control. the complete circuit diagram is given in figure 2. this section briefly explains how the required functionality is implemented by the board 's logic . ( 21k .ps.z ) ( 85k .ps.z ) 3.1. barrier/interrupt hardware although the dbm ( dynamic barrier mimd ) architecture presented in [ cod94 ] [ cod94b ] is far superior to the original dbm design as presented in [ okd90a ] , neither one is simple enough to be efficiently implemented without using programmable logic devices. thus , ttl_papers uses a variation on the sbm ( static barrier mimd ) design of [ okd90 ] . the primary difference between the previously published sbm and the ttl_papers mechanism is that there are two barrier trees rather than one. the reason is simply that the published sbm silently assumed that the barrier hardware would be reset between barriers , essentially by an " anti barrier. " in contrast , the use of two trees allows the hardware for one tree to be reset as a side-effect of the other tree being used , halving the number of operations needed per barrier. both of these two trees are trivially implemented using a 74ls20 , a dual 4-input nand. the result is latched by setting or resetting a 1-bit register implemented using 1/2 of a 74ls74 . the interrupt logic is remarkably similar to the barrier logic , also using a 74ls20 and 1/2 of a 74ls74. however , there is a difference in the connection between these chips . interrupt requests are generated by any pe , and acknowledged by all pes , while barrier synchronizations are always implemented by all pes. thus , to invert the sense of the interrupt request output from the nand , the interrupt logic uses the simple trick of clocking the 1-bit register rather than setting it asynchronously. this trick saves a chip and actually yields a slightly more robust interrupt mechanism , because the interrupt is triggered by an edge rather than by a level . finally , because there are not enough input bits for each pe , the above two latched values must be independently selectable for each pe. the obvious way to select between values is using a multiplexor ; however , there is no standard ttl 74ls-series part that implements selection between two individual bits. instead , we construct the multiplexor using two quad driver chips which have outputs that can be independently set to a high-impedance state. the 74ls125 and 74ls126 differ only in the sense of their enable lines ; thus , using the same select line to control one output on each chip makes it possible to simply wire both outputs together. only the selected signal will be passed ; the other line 's driver will be in the high impedance state . notice also that this logic trivially can be scaled to larger machines. every 4 pes will require a 74ls125 and a 74ls126. the 74ls74 remains unchanged , although a driver chip may be needed to increase fan-out of the outputs for more than 8 processors. the 74ls20 nand gates simply get replaced by larger nand trees. for example , for 8 processors , each 1/2 of a 74ls20 is replaced by a 74ls30 ( 8 input nand ) . for 16 processors , chip count is minimized by implementing each nand tree using a 74ls134 ( 12-input nand ) and 1/2 of a 74ls20 , the outputs of which are combined using 1/4 of a 74ls32 ( quad 2-input or ) . a somewhat neater 16 processor board layout results from using two 74ls30 and 1/4 of a 74ls32 for each of the nand trees , and a 32 processor system can easily be constructed using four 74ls30 and 3/4 of a 74ls32 for each nand tree . although the circuitry scales to very large configurations , for a cluster containing more than 32 machines , the circuit complexity is high enough to warrant using a more sophisticated design based on programmable logic components . 3.2. aggregate communication hardware although other versions of papers provide internal data latching and fancy communication primitives , ttl_papers simply offers nanding of the 4 data bits across the processors . in general , there is nothing tricky about building these nand trees ; they look exactly as described above. however , each nand tree has an output that must drive one line to each of the processors , and the use of relatively long cables ( up to about 10 feet each ) requires a pretty good ttl driver. for the case of 4 processors , we have experimentally confirmed that the 74ls40 ( dual 4-input nand buffer ) provides sufficient drive to directly power all 4 lines , although the signal transitions become somewhat slow ( e.g. , about 300 nanoseconds ) as the cables get long. in a larger system , we recommend constructing the nand tree as described above , and then using 74ls244 or 74ls541 octal drivers to increase the drive ability of the nand outputs . with a typical cable , each driver within a 74ls244 or 74ls541 can drive up to about 4 lines . 3.3. led display hardware in the papers prototypes , we have experimented with a variety of different status displays. however , by far the easiest display to understand was one using just a single bi-color led for each processor. the color code is very intuitive : green means running , red means waiting ( for more than two " clock " cycles ) , and black means not in use. the problem is that there is no trivial way to derive these color choices directly from the barrier logic , thus , the leds are explicitly set under software control. there is also a power light on the ttl_papers unit , which is a blue led to avoid confusion with the rest of the status display . 4. papers software although ttl_papers will be supported by a variety of software tools including public domain compilers for parallel dialects of both c and fortran [ dio92 ] [ cod94a ] , in this document we restrict our discussion to the most basic hardware-level interface. the code given is written in c ( the ansi c-based dialect accepted by gcc ) and is intended to be run under a unix-derived operating system. however , this interface software can be adapted to most existing ( sequential ) language compilers and interpreters under nearly any operating system . the following sections discuss the operating system interface , ttl_papers port access , the basic barrier interface , and how nanding is used to implement data communication . 4.1. operating system interface although it would certainly be possible to implement the ttl_papers software interface as part of an operating system 's kernel , typical latency for a minimal system call is between 5 and 50 times the typical latency for the ttl_papers hardware port operations -- layering overhead of a system call for each ttl_papers operation would destroy the low latency performance. thus , the primary purpose of the os interface is to obtain direct user-level access to the ports that connect to ttl_papers . on older architectures , such as the 8080 and z80 , direct user access to i/o was accomplished by simply using the port i/o instructions or by accessing the memory locations that corresponded to the desired memory-mapped i/o device registers. things are now a bit more complex. using a pc based on the 386 , 486 , or pentium processor , port i/o instructions are now protected operations. similarly , the dec alpha , ibm powerpc , and sun sparc use memory mapped i/o , but physical addresses corresponding to i/o ports generally are not mapped into the virtual address space of a user process. none of these problems is fatal , but it can take quite a while to figure out how to get things working right . thus , although the parallel printer port can be directly accessed under most operating systems , here we focus on how to gain direct user process access to i/o ports on a 386 , 486 , or pentium-based personal computer running either generic unix or linux . 4.1.1. generic unix in general , unix allows user processes to have direct access to all i/o devices. however , only processes that have a sufficiently high i/o priority level can make such accesses . further , only a privileged process can increase its i/o priority level -- by calling iopl ( ) . the following c code suffices : if ( iopl ( 3 ) ) { / * iopl failed , implying we were not priv * / exit ( 1 ) ; } however , this call grants the user program access to all i/o , including a multitude of unrelated ports . in fact , this call allows the process to execute instructions enabling and disabling interrupts. by disabling interrupts , it is possible to ensure that all processors involved in a barrier synchronization act precisely in unison ; thus , the average number of port operations ( barrier synchronizations ) needed to accomplish some ttl_papers operations can be reduced. however , background scheduling of dma devices ( e.g. , disks ) and other interference makes it hard to be sure that a unix system will provide precise timing constraints even when interrupts are disabled , so we do not advocate disabling interrupts . even so , performance of the barrier hardware can be safely improved by causing unix to give priority to a process that is waiting for a barrier synchronization. this improves performance because if any one pe is off running a process that has nothing to do with the synchronization , then all pes trying to synchronize with that pe will be delayed. the priority of a privileged unix process can increased by a call to nice ( ) with a negative argument between -20 and -1 . 4.1.2. linux although linux supports the unix interface described in the previous section , it also provides a more secure way to obtain access to the i/o devices. the ioperm ( ) function allows a privileged process to obtain access to only the specified port or ports. the c code : if ( ioperm ( p_portbase , 3 , 1 ) ) { / * like iopl , failure implies we were not priv * / exit ( 1 ) ; } obtains access for 3 ports starting at a base port address of p_portbase . better still , if the operating system is managing all parallel program interrupts , only the first two ports need to be accessible : if ( ioperm ( p_portbase , 2 , 1 ) ) { / * like iopl , failure implies we were not priv * / exit ( 1 ) ; } because the 386/486/pentium hardware checks port permissions , this security does not destroy port i/o performance ; however , checking the permission bits does add some overhead. for a typical pc parallel printer port , the additional overhead is just a few percent , and is probably worthwhile for user programs . 4.2. port access although linux and most versions of unix provide routines for port access , these routines often provide a built-in delay loop to ensure that port states do not change faster than the external device can examine the state . consequently , the ttl_papers support code uses its own direct assembly language i/o calls. the code is : inline unsigned int inb ( unsigned short port ) { unsigned char _v ; __asm__ __volatile__ ( " inb % w1 , % b0 " : " = a " ( _v ) : " d " ( port ) , " 0 " ( 0 ) ) ; return ( _v ) ; } inline void outb ( unsigned char value , unsigned short port ) { __asm__ __volatile__ ( " outb % b0 , % w1 " : / * no outputs * / : " a " ( value ) , " d " ( port ) ) ; } the basic ttl_papers interface is thus defined in terms of the above port operations on any of three parallel printer port interface registers. the following definitions assume that p_portbase has already been set to the base i/o address for the port , which is generally 0x378 on pc clones and 0x3bc for ibm valuepoint pcs . / * to output to p_portbase * / # define p_out ( x ) \ outb ( ( ( unsigned char ) ( x ) ) , \ ( ( unsigned short ) p_portbase ) ) / * to input from p_portbase + 1 * / # define p_in ( ) \ inb ( ( unsigned short ) ( p_portbase + 1 ) ) / * to output to p_portbase + 2 * / # define p_mode ( x ) \ outb ( ( ( unsigned char ) ( x ) ) , \ ( ( unsigned short ) ( p_portbase + 2 ) ) ) it is important to note that , even though the ttl_papers hardware interrupt mechanism does not have to be used , it is necessary that any processor connected to ttl_papers set the mode port so that the p_rdy bit , and not p_int , is visible . this can be done by executing : p_mode ( p_nak ) ; as part of the ttl_papers initialization code . 4.3. barrier interface logically , each barrier synchronization consists of two operations : signaling that we are at a barrier and waiting to be signaled that the barrier synchronization has completed . although the ttl_papers library generally combines these operations , here we discuss them as two separate chunks of code. the code for signaling that we are at a barrier is simply : p_out ( last_out ^ = ( p_s0 | p_s1 ) ) ; this code just flips the sense of both strobe bits. because last_out is initialized to have only one of the strobe bits high , this has the effect of alternating between p_s0 and p_s1 . nothing else is changed , including the output data bits . the code that actually waits for the barrier synchronization to complete is larger than one might expect , even though typically only a few instructions are executed. the reason that there is so much code has to do with three features of the ttl_papers interface. the first complication is that the ready signal toggles , rather than always going to the same value. the second complication is that the status leds are software controlled. the third complication is that we want to check for an interrupt whenever a barrier is excessively delayed. the resulting code is something like : / * which condition am i waiting for ? * / if ( last_out & p_s0 ) { / * waiting for p_rdy * / if ( ( ! ( p_in ( ) & p_rdy ) ) && ( ! ( p_in ( ) & p_rdy ) ) ) { / * polled twice , make led red * / p_out ( last_out ^ = ( p_lg | p_lr ) ) ; / * continue waiting * / while ( ! ( p_in ( ) & p_rdy ) ) checkint ; / * ok , led green again * / p_out ( last_out ^ = ( p_lg | p_lr ) ) ; } } else { / * waiting for not p_rdy * / if ( ( p_in ( ) & p_rdy ) && ( p_in ( ) & p_rdy ) ) { / * polled twice , make led red * / p_out ( last_out ^ = ( p_lg | p_lr ) ) ; / * continue waiting * / while ( p_in ( ) & p_rdy ) checkint ; / * ok , led green again * / p_out ( last_out ^ = ( p_lg | p_lr ) ) ; } } in the initial version of the support library , checkint is nothing -- ttl_papers interrupts are not used. however , we can easily check for an interrupt by defining checkint as : { / * make p_int status visible * / p_mode ( p_sel | p_nak ) ; / * check for interrupt * / if ( p_in ( ) & p_int ) { / * process the interrupt.... * / } else { / * restore p_rdy * / p_mode ( p_nak ) ; } } 4.4. nand data communication although the ttl_papers library provides a rich array of aggregate communication operations , all that the hardware really does is a simple 4-bit nand as a side-effect of a barrier synchronization. however , this operation typically requires 5 port accesses rather than just the 2 port accesses used to implement a barrier synchronization without data communication. the extra port operations are required because : when a pe changes one or more of its output data bits , the ttl_papers hardware nands in the new data , immediately changing the input data bits for all pes. thus , if one pe gets far enough ahead of another pe , it could change the data before the slower pe has been able to read the previous nand result. if interrupts are disabled and an initial ordinary barrier synchronization is performed , then a block of data can be transmitted safely using static timing analysis alone to ensure that this race condition does not occur. unfortunately , it is n't practical to disable interrupts under unix , so the safe solution is to follow each data transmitting barrier with an ordinary barrier that ensures all pes have read the data before any pe can change the data. this adds 2 port operations . the ttl_papers hardware is fed both data and strobe signals on the same port. thus , data and strobe signals change simultaneously. as they race through the cables to the ttl_papers logic and back out the cables , the ttl_papers logic should give the nand data at least a 20 nanosecond lead over the toggling of the p_rdy bit , but is 20 nanoseconds enough to ensure that the nand data does n't lose the race ? well , it depends on the implementation of the pc port hardware and the electrical properties of the cables.. . which is another way of saying that the race is n't acceptable. an additional port operation is used to ensure that the data wins the race . there are two possible ways to insert the extra operation. one way is to double the output operation , so that we first change the data and then toggle the strobe . this has the advantage that only pes that are actually changing their data would need to insert the extra operation. if only early pes change their data , we do n't see any delay ; however , the asymmetry of the pe operations can actually result in more delay than if the extra operation was always inserted. the other alternative is to resample the nand data value after p_rdy has signaled it is present. this is somewhat more reliable than the doubling of the output operation , but the delay is always present for any pe that reads the data . in any case , the result is that a barrier synchronization accompanied by an aggregate communication will take 5 port operations. for example , to perform a reliable nand with our contribution being the 4-bit value x , we could first : last_out = ( ( last_out & 0xf0 ) | x ) ; this sets the new data bits so that a barrier synchronization will transmit them along with the strobe . the next step is to perform a barrier synchronization , precisely as described in section 4.3. having completed the barrier , we know that the nand data should now be valid , so we resample it : nand _ result = p_in ( ) ; finally , a second barrier synchronization is performed to ensure that no pe changes its output data until after everyone has read the current nand data . notice that nand _ result is actually an 8-bit value with the nand data embedded within it ; thus , to extract the 4-bit result we simply use : ( ( nand _ result all the ttl_papers library communication operations are built upon this communication mechanism. for example , any , all , and voting operations require just one such operation , or 5 port accesses. larger operations , such as an 8-bit global or , 8-bit global and , or broadcast require 2 of the above transmission sequences , or 10 port accesses . 5. performance the performance of the basic ttl_papers operations is summarized in the table below. these numbers were obtained using 486dx33-based ibm valuepoint pcs running linux version 1.1.75. machines with faster printer ports obtain correspondingly higher performance ; in fact , performance can be increased by nearly an order of magnitude without any other changes . ( 4k .ps.z ) the primary observation is that the total unix-process to unix-process latency is remarkably low. for example , we benchmarked a 64-processor ncube2 at 206 milliseconds for a simple barrier synchronization -- 83,000 times slower than the ttl_papers cluster ! in fact , the minimum latency for sending a single message on the ncube2 was reported to be between 32 and 110 microseconds [ brg94 ] , which is between 13 and 44 times as long as it takes for a ttl_papers barrier synchronization. the same paper [ brg94 ] quotes that the intel paragon xp/s has a minimum message latency of 240 microseconds , which is 97 times the ttl_papers barrier latency. of course , any conventional workstation network will have a minimum message latency of at least hundreds of microseconds , simply due to the overhead of unix context switching and the socket protocol . the second observation is that the bandwidth is not very impressive. however , bandwidth for ttl_papers is n't really measuring the same thing as bandwidth for a conventional network. if you want to perform asynchronous block transfers , ttl_papers can not compete with a conventional network. however , the bandwidth for the aggregate operations ( listed above ) is much higher for ttl_papers than for nearly any other network attempting to synthesize these operations. in summary , if a program needs to perform an aggregate operation , use ttl_papers. if it needs to do some other type of communication , use the conventional network -- having ttl_papers on a cluster does not interfere with any other network 's operation . 6. conclusion in this paper , we have detailed the design of the ttl_papers hardware. this public domain design represents the simplest possible mechanism to efficiently support barrier synchronization , aggregate communication , and group interrupt capabilities -- using unmodified conventional workstations or personal computers as the processing elements of a fine-grain parallel machine. we do not view ttl_papers as the ultimate mechanism , but rather as an introductory step toward the more general and higher performance barrier and aggregate communication engines that have been at the core of our research since 1987. the key thing to remember about ttl_papers is not what it is , but rather why it is . why is ttl_papers so much lower latency than other networks ? because it does n't have a layered hardware and software interface. why is the hardware so simple ? because it is n't a network ; the fact that ttl_papers communications are a side-effect of barrier synchronization eliminates the need for buffering , routing , arbitration , etc. why is it useful ? because , although shared memory and message passing hardware is very common , the most popular high-level language and compiler models for parallelism are all based on aggregate operations -- exactly what ttl_papers provides. further , barrier synchronization is the key to efficiently implementing mimd , simd , and vliw mixed-mode execution. why did n't somebody do it earlier ? we and others did. the problem is that tightly coupled design of hardware and compiler is not the standard way to build systems , so earlier designs ( e.g. , pasm , tmc cm-5 ) tended to use too much hardware and interface software , cost too much , and perform too poorly . higher-performance versions of papers are on the way. in fact , this ttl_papers design is the sixth papers design we have built and tested , and three of the other designs easily outperform it... but they use much more hardware. we are currently working on a smart parallel port card for the isa bus that will roughly quadruple the performance of ttl_papers without any change to its hardware. we are also pursuing versions of the higher-performance designs based on texas instruments fpgas , and anticipate a pci interface to future papers units. also watch for releases of various compilers targeting papers. the www url http : //garage.ecn.purdue.edu/ ~ papers/ will be the primary place for announcing and distributing future releases of both hardware and software . references [ brg94 ] u. bruening , w. k. giloi , and w. schroeder-preikschat , " latency hiding in message-passing architectures , " 8th international parallel processing symposium , cancun , mexico , april , 1994 , pp. 704-709 . [ cod94 ] w. e. cohen , h. g. dietz , and j. b. sponaugle , dynamic barrier architecture for multi-mode fine-grain parallelism using conventional processors ; part i : barrier architecture , purdue university school of electrical engineering , technical report tr-ee 94-9 , march 1994 . [ cod94a ] w. e. cohen , h. g. dietz , and j. b. sponaugle , dynamic barrier architecture for multi-mode fine-grain parallelism using conventional processors ; part ii : mode emulation , purdue university school of electrical engineering , technical report tr-ee 94-10 , march 1994 . [ cod94b ] w. e. cohen , h. g. dietz , and j. b. sponaugle , " dynamic barrier architecture for multi-mode fine-grain parallelism using conventional processors , " proc. of 1994 int'l conf. on parallel processing , st. charles , il , pp. i 93-96 , august 1994 . [ cra93 ] cray t3d system architecture overview , publication hr-04033 , cray research , inc. , 2360 pilot knob road , mendota heights , mn 55120 , 1993 . [ dic94 ] h. g. dietz , w. e. cohen , t. muhammad , and t. i. mattox , " compiler techniques for fine-grain execution on workstation clusters using papers , " 7th annual workshop on languages and compilers for parallel computing ( also to appear as a book chapter from springer verlag ) , cornell university , august 1994 . [ dim94 ] h. g. dietz , t. muhammad , j. b. sponaugle , and t. mattox , papers : purdue 's adapter for parallel execution and rapid synchronization , purdue university school of electrical engineering , technical report tr-ee 94-11 , march 1994 . [ dio92 ] h. g. dietz , m.t. o'keefe , and a. zaafrani , " static scheduling for barrier mimd architectures , " the journal of supercomputing , vol. 5 , pp. 263-289 , 1992 . [ jor78 ] h. f. jordon , " a special purpose architecture for finite element analysis , " proc. int'l conf. on parallel processing , pp. 263-266 , 1978 . [ okd90 ] m. t. o'keefe and h. g. dietz , " hardware barrier synchronization : static barrier mimd ( sbm ) , " proc. of 1990 int'l conf. on parallel processing , st. charles , il , pp. i 35-42 , august 1990 . [ okd90a ] m. t. o'keefe and h. g. dietz , " hardware barrier synchronization : dynamic barrier mimd ( dbm ) , " proc. of 1990 int'l conf. on parallel processing , st. charles , il , pp. i 43-46 , august 1990 . [ sin87 ] t. schwederski , w. g. nation , h. j. siegel , and d. g. meyer , " the implementation of the pasm prototype control hierarchy , " proc. of second int'l conf. on supercomputing , pp. i 418-427 , 1987 . hypertext index abstract keywords notice for html users 1. introduction 1.1. synchronization 1.2. communication 1.3. interrupts 2. pc hardware 2.1. pe hardware interface 2.2. pe port bit assignments 3. ttl_papers hardware 3.1. barrier/interrupt hardware 3.2. aggregate communication hardware 3.3. led display hardware 4. papers software 4.1. operating system interface 4.1.1. generic unix 4.1.2. linux 4.2. port access 4.3. barrier interface 4.4. nand data communication 5. performance 6. conclusion references hypertext index the only thing set in stone is our name .
